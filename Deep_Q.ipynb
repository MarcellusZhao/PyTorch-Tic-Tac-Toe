{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "mrrejmZfnufq",
      "metadata": {
        "id": "mrrejmZfnufq"
      },
      "source": [
        "<div class=\"alert alert-warning\" role=\"alert\">\n",
        "    <h4 class=\"alert-heading\">Reminder!</h4>\n",
        "    <p>Our team uses Plotly to visualise reward in a continuous manner, therefore:</p>\n",
        "    <ul>\n",
        "      <li>If you have Plotly and ipywidgets installed in your environment, skip the following code cell</li>\n",
        "      <li>Otherwise, run the following code cell and <strong>restart your jupyter notebook!</strong></li>\n",
        "    <ul>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f3a9b26",
      "metadata": {
        "id": "8f3a9b26"
      },
      "outputs": [],
      "source": [
        "!pip install plotly\n",
        "!pip install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "423fa7e1",
      "metadata": {
        "id": "423fa7e1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "\n",
        "from tic_env import *\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import plotly.io as pio\n",
        "from plotly.subplots import make_subplots"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O8JZ3N6xqIE_",
      "metadata": {
        "id": "O8JZ3N6xqIE_"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c407759",
      "metadata": {
        "id": "4c407759"
      },
      "outputs": [],
      "source": [
        "colours = px.colors.qualitative.Plotly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t0gCXm6ncZIn",
      "metadata": {
        "id": "t0gCXm6ncZIn"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "52aff524",
      "metadata": {
        "id": "52aff524"
      },
      "outputs": [],
      "source": [
        "def render_figure_widge(mode=None):\n",
        "    gofig = go.FigureWidget()\n",
        "    gofig.update_xaxes(title_text='Epoch', autorange=True)\n",
        "    gofig.update_yaxes(title_text='Avg. Reward', autorange=True)\n",
        "    \n",
        "    if mode == 'validation':\n",
        "        gofig.update_yaxes(title_text='Diff. Rate', autorange=True)\n",
        "        \n",
        "    gofig.update_layout(width=1200, height=800, hovermode=\"x unified\")\n",
        "    \n",
        "    # Only for question 11 and 12\n",
        "    if mode == 'Reward+Loss':\n",
        "        gofig = make_subplots(rows=1, cols=2, subplot_titles=['Average Reward', 'Training Loss'])\n",
        "        gofig.update_layout(width=1400, height=500, hovermode=\"x unified\")\n",
        "        gofig = go.FigureWidget(gofig)\n",
        "        \n",
        "    return gofig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "24ef2e01",
      "metadata": {
        "id": "24ef2e01"
      },
      "outputs": [],
      "source": [
        "def init_figure_widge(fig_widge, mode, colour, epsilon_opt=None, epsilon_learn=None, n_star=None):\n",
        "    # Mode epsilon only used for question 11\n",
        "    if mode == 'epsilon':\n",
        "        if not (epsilon_learn >= 0 and epsilon_learn < 1):\n",
        "            raise Exception('Epsilon value empty or out of range')\n",
        "        \n",
        "        # For average reward - 0\n",
        "        fig_widge.add_trace(go.Scatter(x=[0], y=[0], name=f'Avg. Reward = {epsilon_learn}', marker=dict(color=colour)), row=1, col=1)\n",
        "        \n",
        "        # For training loss - 1\n",
        "        fig_widge.add_trace(go.Scatter(x=[0], y=[0], name=f'Tr. Loss = {epsilon_learn}', marker=dict(color=colour)), row=1, col=2)\n",
        "        \n",
        "    # Mode 'descrease exploration' only used for question 12\n",
        "    elif mode == 'decrease exploration':\n",
        "        if not isinstance(n_star,int):\n",
        "            raise TypeError('Cannot find or recognise n_star')       \n",
        "        \n",
        "        # For average reward - 0\n",
        "        fig_widge.add_trace(go.Scatter(x=[0], y=[0], name=f'Avg. Reward = {n_star}', marker=dict(color=colour)), row=1, col=1)\n",
        "        \n",
        "        # For training loss - 1\n",
        "        fig_widge.add_trace(go.Scatter(x=[0], y=[0], name=f'Tr. Loss = {n_star}', marker=dict(color=colour)), row=1, col=2)\n",
        "        \n",
        "    elif mode == 'validate n*':\n",
        "        fig_widge.add_scatter(x=[0], y=[0], name=f'M_opt n* = {n_star}', marker=dict(color=colour))\n",
        "        fig_widge.add_scatter(x=[0], y=[0], name=f'M_rand n* = {n_star}', marker=dict(color=colour), line=dict(dash='dot'))\n",
        "        \n",
        "    elif mode == 'validate epsilon opt':\n",
        "        fig_widge.add_scatter(x=[0], y=[0], name=f'M_opt epilson opt= {epsilon_opt}', marker=dict(color=colour))\n",
        "        fig_widge.add_scatter(x=[0], y=[0], name=f'M_rand epilson opt= {epsilon_opt}', marker=dict(color=colour), line=dict(dash='dot'))\n",
        "        \n",
        "    else:\n",
        "        raise NotImplementedError(\"Unable to recognise mode\")\n",
        "            \n",
        "    return fig_widge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "181888d4-32fc-4b99-bd11-7d9b2606b5c0",
      "metadata": {
        "id": "181888d4-32fc-4b99-bd11-7d9b2606b5c0"
      },
      "outputs": [],
      "source": [
        "def check_available(grid, pos):\n",
        "    \"\"\"Input grid should be 2 * 3 * 3 tensor\"\"\"\n",
        "    x, y = pos // 3, pos % 3\n",
        "    if grid[0,x,y] == grid[1,x,y] == 0:\n",
        "        return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "c1a01956-b778-4012-a038-62f4b2f5e011",
      "metadata": {
        "id": "c1a01956-b778-4012-a038-62f4b2f5e011"
      },
      "outputs": [],
      "source": [
        "def grid2state(grid, learner_value):\n",
        "    grid2d = np.tile(grid, (2,1)).reshape(2,3,3)\n",
        "    if learner_value == \"X\":\n",
        "        grid2d[0] = np.clip(grid2d[0], 0, 1)\n",
        "        grid2d[1] = np.clip(-grid2d[1], 0, 1)\n",
        "    else:\n",
        "        grid2d[0] = np.clip(-grid2d[0], 0, 1)\n",
        "        grid2d[1] = np.clip(grid2d[1], 0, 1)\n",
        "    return torch.Tensor(grid2d).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "2ae96edd",
      "metadata": {
        "id": "2ae96edd"
      },
      "outputs": [],
      "source": [
        "def validation(epsilon_opt, epsilon_learn, DeepQLearner, epoch=500):\n",
        "    env = TictactoeEnv()\n",
        "    Turns = np.array(['O','X'])\n",
        "    reward_sum = 0\n",
        "    \n",
        "    for i in range(epoch):\n",
        "        np.random.seed(i)\n",
        "        \n",
        "        # Our policy makes the first move in the first 250 games\n",
        "        if i < 250:\n",
        "            Turns = np.array(['O','X'])\n",
        "        else:\n",
        "            Turns = np.array(['X','O'])\n",
        "        \n",
        "        env.reset()\n",
        "        grid, _, __ = env.observe()\n",
        "        player_opt = OptimalPlayer(epsilon=epsilon_opt, player=Turns[0])\n",
        "        player_learn = DeepQLearner\n",
        "        player_learn.epsilon = epsilon_learn\n",
        "        player_learn.player = Turns[1]\n",
        "\n",
        "        for j in range(9):\n",
        "            if env.current_player == player_opt.player:\n",
        "                action = player_opt.act(grid)\n",
        "                grid, end, winner = env.step(action, print_grid=False)\n",
        "            else:\n",
        "                state = grid2state(grid, player_learn.player).to(DEVICE)\n",
        "                action = player_learn.act(state)\n",
        "                # new_position = player_learn.act(grid, q_library)\n",
        "                if check_available(grid2state(grid, player_learn.player), action.item()):\n",
        "                    grid, end, winner = env.step(action.item(), print_grid=False)\n",
        "                else:\n",
        "                    # End the game if the agent takes an unavailable action\n",
        "                    end = True\n",
        "                    # Give the agent a negative reward\n",
        "                    reward = -1\n",
        "                    \n",
        "                    # # Opt wins the games\n",
        "                    env.winner = player_opt.player\n",
        "\n",
        "            if end:\n",
        "                reward_sum += env.reward(player=player_learn.player)\n",
        "                env.reset()\n",
        "                break\n",
        "                \n",
        "    return reward_sum/epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "z35OU4uX9eP5",
      "metadata": {
        "id": "z35OU4uX9eP5"
      },
      "outputs": [],
      "source": [
        "def run_simulation(epsilon_opt, buffer_capacity=10000, batch_size=64, epoch=20000, plot_interval=250, \n",
        "                   colour='black', epsilon_learn_init=None, n_star=None, fig_widge=None, mode=None):\n",
        "\n",
        "    any_epsilon = (epsilon_learn_init is None) and (n_star is None)\n",
        "    assert not any_epsilon, \"Both epsilon_learn and n_star is None! At least having one of them.\"\n",
        "\n",
        "    env = TictactoeEnv()        \n",
        "    Turns = np.array(['X','O'])\n",
        "    plot_interval = 250\n",
        "\n",
        "    # rendering in figure widge\n",
        "    if fig_widge is not None:\n",
        "        init_figure_widge(fig_widge, mode, colour, \n",
        "                          epsilon_opt=epsilon_opt, \n",
        "                          epsilon_learn=epsilon_learn_init, \n",
        "                          n_star=n_star)\n",
        "        \n",
        "    replay_memory = Buffer(capacity=buffer_capacity, batch_size=batch_size)\n",
        "    player_opt = OptimalPlayer(epsilon=epsilon_opt, player=Turns[0])\n",
        "    player_learn = DeepQAgent(epsilon=epsilon_learn_init, player=Turns[1])\n",
        "\n",
        "    epsilon_min = 0.1\n",
        "    epsilon_max = 0.8\n",
        "\n",
        "    reward_sum = 0\n",
        "    loss_sum = 0\n",
        "    loss_step = 0\n",
        "    average_rewards = []\n",
        "    average_losses = []\n",
        "\n",
        "    for i in range(epoch):\n",
        "        # if mode == 'decrease exploration' or 'validation':\n",
        "        if epsilon_learn_init is None:\n",
        "            assert n_star is not None, \"In this setting, we use n_star to tune the epsilon_learn.\"\n",
        "            epsilon_learn = max(epsilon_min, epsilon_max*(1-(i+1)/n_star))\n",
        "        else:\n",
        "            epsilon_learn = epsilon_learn_init\n",
        "        \n",
        "        player_learn.epsilon = epsilon_learn\n",
        "\n",
        "        env.reset()\n",
        "        grid, _, _ = env.observe()\n",
        "\n",
        "        # Switch Order\n",
        "        Turns = Turns[::-1]\n",
        "        player_opt.player = Turns[0]\n",
        "        player_learn.player = Turns[1]\n",
        "        env.current_player  = 'X'\n",
        "\n",
        "        state = None\n",
        "        next_state = None\n",
        "\n",
        "        for j in range(9):\n",
        "            if env.current_player == player_opt.player:\n",
        "                action_opt = player_opt.act(grid)\n",
        "                grid, end, winner = env.step(action_opt, print_grid=False)\n",
        "                \n",
        "                # Get reward\n",
        "                reward = env.reward(player=player_learn.player)\n",
        "                \n",
        "                # In case opt plays first\n",
        "                if j > 0:\n",
        "                    next_state = grid2state(grid, player_learn.player)\n",
        "            else:\n",
        "                state = grid2state(grid, player_learn.player)\n",
        "                action = player_learn.act(state)\n",
        "                action = action.to(DEVICE)\n",
        "\n",
        "                # Check the availability of current action.\n",
        "                if check_available(grid2state(grid, player_learn.player), action.item()):\n",
        "                    grid, end, winner = env.step(action.item(), print_grid=False)\n",
        "\n",
        "                    # Get reward.\n",
        "                    reward = env.reward(player=player_learn.player)\n",
        "                else:\n",
        "                    # End the game if the agent takes an unavailable action\n",
        "                    end = True\n",
        "                    unavailable_action = True\n",
        "                    # Give the agent a negative reward\n",
        "                    reward = -1\n",
        "                    \n",
        "                    # Opponent wins the games\n",
        "                    env.winner = player_opt.player\n",
        "\n",
        "                    \n",
        "            if not end:\n",
        "                # In case opt players first - next_state does not exist\n",
        "                if next_state != None:\n",
        "                    replay_memory.push(state.unsqueeze(0), action, next_state, torch.tensor([reward], device=DEVICE))\n",
        "                    if len(replay_memory) >= replay_memory.batch_size:\n",
        "                        loss = player_learn.train(replay_memory)\n",
        "                        loss_step += 1\n",
        "                        loss_sum += loss\n",
        "                    next_state = None\n",
        "                    \n",
        "            if end:\n",
        "                # Once the game ends, no matter which player plays first\n",
        "                # Update is the same.\n",
        "                if env.winner == player_opt.player:\n",
        "                    # If opt wins the game, reward is guaranteed to be update-to-date\n",
        "                    reward = -1\n",
        "                elif env.winner == player_learn.player:\n",
        "                    # our agent wins.\n",
        "                    reward = 1\n",
        "                else:\n",
        "                    # Draw\n",
        "                    reward = 0\n",
        "\n",
        "                next_state = None\n",
        "                # Update target model every 500 epoch\n",
        "                update_target = False\n",
        "\n",
        "                if (i+1) % 500 == 0:\n",
        "                    update_target = True\n",
        "                replay_memory.push(state.unsqueeze(0), action, next_state, torch.tensor([reward], device=DEVICE))\n",
        "                if len(replay_memory) >= replay_memory.batch_size:\n",
        "                    loss = player_learn.train(replay_memory, update_target)\n",
        "                    loss_step += 1\n",
        "                    loss_sum += loss\n",
        "                    \n",
        "                reward_sum += reward\n",
        "                env.reset()\n",
        "                break       \n",
        "            \n",
        "        #############################\n",
        "        ######### Plot ##############\n",
        "\n",
        "        if (i+1) % plot_interval == 0:\n",
        "            # calculate average reward at the end of the current interval.\n",
        "            average_reward = reward_sum / plot_interval  \n",
        "            average_loss = loss_sum / loss_step\n",
        "            # print(average_reward)  \n",
        "            # print(average_loss)\n",
        "            \n",
        "            if not 'validate' in mode:\n",
        "                idx = len(fig_widge.data)-2\n",
        "                \n",
        "                fig_widge.data[idx].x = np.append(fig_widge.data[idx].x, i+1)[0:]\n",
        "                fig_widge.data[idx].y = np.append(fig_widge.data[idx].y, average_reward)[0:]\n",
        "                \n",
        "                fig_widge.data[idx+1].x = np.append(fig_widge.data[idx+1].x, i+1)[0:]\n",
        "                fig_widge.data[idx+1].y = np.append(fig_widge.data[idx+1].y, average_loss.item())[0:]\n",
        "                \n",
        "            else:\n",
        "                M_opt = validation(0, 0, player_learn, epoch=500)\n",
        "                M_rand = validation(1, 0, player_learn, epoch=500)\n",
        "                \n",
        "                idx = len(fig_widge.data)-2\n",
        "                fig_widge.data[idx].x = np.append(fig_widge.data[idx].x, i+1)[0:]\n",
        "                fig_widge.data[idx].y = np.append(fig_widge.data[idx].y, M_opt)[0:]\n",
        "                fig_widge.data[idx+1].x = np.append(fig_widge.data[idx+1].x, i+1)[0:]\n",
        "                fig_widge.data[idx+1].y = np.append(fig_widge.data[idx+1].y, M_rand)[0:]\n",
        "                fig_widge.layout.title.text = f'Epoch {i+1}, M_opt = {M_opt}, M_rand = {M_rand}'\n",
        "\n",
        "            average_rewards.append(average_reward)\n",
        "            average_losses.append(average_loss)\n",
        "            # reset reward_sum.\n",
        "            reward_sum = 0\n",
        "            loss_sum = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "6c0d6a0c-a019-47cf-ab1c-16e50c243df2",
      "metadata": {
        "id": "6c0d6a0c-a019-47cf-ab1c-16e50c243df2"
      },
      "outputs": [],
      "source": [
        "def self_practice_simulation(epsilon_opt=None, epoch=20000, plot_interval=250, \n",
        "                   colour='black', epsilon_learn_init=None, n_star=None, fig_widge=None, mode=None):\n",
        "\n",
        "    any_epsilon = (epsilon_learn_init is None) and (n_star is None)\n",
        "    assert not any_epsilon, \"Both epsilon_learn and n_star is None! At least having one of them.\"\n",
        "\n",
        "    env = TictactoeEnv()        \n",
        "    Turns = np.array(['X','O'])\n",
        "    plot_interval = 250\n",
        "\n",
        "    # rendering in figure widge\n",
        "    if fig_widge is not None:\n",
        "        if n_star is None:\n",
        "            fig_widge.add_scatter(x=[0], y=[0], name=f'M-opt vs. Q-agent, epsilon = {epsilon_learn_init}', marker=dict(color=colour))\n",
        "            fig_widge.add_scatter(x=[0], y=[0], name=f'M-rand vs. Q-agent, epsilon = {epsilon_learn_init}', marker=dict(color=colour), line=dict(dash='dot'))\n",
        "        else:\n",
        "            fig_widge.add_scatter(x=[0], y=[0], name=f'M-opt vs. Q-agent, n* = {n_star}', marker=dict(color=colour))\n",
        "            fig_widge.add_scatter(x=[0], y=[0], name=f'M-rand vs. Q-agent, n* = {n_star}', marker=dict(color=colour), line=dict(dash='dot'))\n",
        "        \n",
        "    replay_memory = Buffer(capacity=10000, batch_size=64)\n",
        "    shared_model = FCN()\n",
        "    # Use player_A as our agent. Need to exchange order when using player_B.\n",
        "    player_A = DeepQAgent(epsilon=epsilon_learn_init, player='O', model=shared_model)\n",
        "    player_B = DeepQAgent(epsilon=epsilon_learn_init, player='X', model=shared_model)\n",
        "\n",
        "    epsilon_min = 0.1\n",
        "    epsilon_max = 0.8\n",
        "\n",
        "    reward_sum_A = 0\n",
        "    loss_sum_A = 0\n",
        "    loss_step_A = 0\n",
        "    average_rewards_A = []\n",
        "    average_losses_A = []\n",
        "\n",
        "    for i in range(epoch):\n",
        "        # if mode == 'decrease exploration' or 'validation':\n",
        "        if epsilon_learn_init is None:\n",
        "            assert n_star is not None, \"In this setting, we use n_star to tune the epsilon_learn.\"\n",
        "            epsilon_learn = max(epsilon_min, epsilon_max*(1-(i+1)/n_star))\n",
        "        else:\n",
        "            epsilon_learn = epsilon_learn_init\n",
        "        \n",
        "        player_A.epsilon = epsilon_learn\n",
        "        player_B.epsilon = epsilon_learn\n",
        "\n",
        "        env.reset()\n",
        "        grid, _, _ = env.observe()\n",
        "\n",
        "        # Switch Order\n",
        "        # Turns = Turns[::-1] \n",
        "        # env.current_player = Turns[0]\n",
        "        Turns = Turns[::-1]\n",
        "        player_A.player = Turns[0]\n",
        "        player_B.player = Turns[1]\n",
        "        env.current_player  = 'X'\n",
        "\n",
        "        state_A = None\n",
        "        next_state_A = None\n",
        "        state_B = None\n",
        "        next_state_B = None\n",
        "\n",
        "        for j in range(9):\n",
        "            if env.current_player == player_A.player:\n",
        "                # state_A is observed from the perspective of A.\n",
        "                state_A = grid2state(grid, player_A.player)\n",
        "                action_A = player_A.act(state_A)\n",
        "                action_A = action_A.to(DEVICE)\n",
        "                \n",
        "                # Get reward\n",
        "                if check_available(grid2state(grid, player_A.player), action_A.item()):\n",
        "                    grid, end, winner = env.step(action_A.item(), print_grid=False)\n",
        "\n",
        "                    # Get reward.\n",
        "                    reward_A = env.reward(player=player_A.player)\n",
        "                    # In case A plays first.\n",
        "                    if j > 0:\n",
        "                        next_state_B = grid2state(grid, player_B.player)\n",
        "                else:\n",
        "                    # End the game if the agent takes an unavailable action\n",
        "                    end = True\n",
        "                    # Give the agent a negative reward\n",
        "                    reward_A = -1\n",
        "                    \n",
        "                    # # Opt wins the games\n",
        "                    env.winner = player_B.player\n",
        "\n",
        "            else:\n",
        "                # state_B is observed from the perspective of B.\n",
        "                state_B = grid2state(grid, player_B.player)\n",
        "                action_B = player_B.act(state_B)\n",
        "                action_B = action_B.to(DEVICE)\n",
        "\n",
        "                if check_available(grid2state(grid, player_B.player), action_B.item()):\n",
        "                    grid, end, winner = env.step(action_B.item(), print_grid=False)\n",
        "\n",
        "                    # Get reward.\n",
        "                    reward_B = env.reward(player=player_B.player)\n",
        "                    # In case A plays first\n",
        "                    if j > 0:\n",
        "                        next_state_A = grid2state(grid, player_A.player)\n",
        "                else:\n",
        "                    # End the game if the agent takes an unavailable action\n",
        "                    end = True\n",
        "                    # Give the agent a negative reward\n",
        "                    reward_B = -1\n",
        "                    \n",
        "                    # # Opt wins the games\n",
        "                    env.winner = player_A.player\n",
        "    \n",
        "            if not end:\n",
        "                # In case opt players first - next_state does not exist\n",
        "                if next_state_A != None:\n",
        "                    replay_memory.push(state_A.unsqueeze(0), action_A, next_state_A, torch.tensor([reward_A], device=DEVICE))\n",
        "                    if len(replay_memory) >= replay_memory.batch_size:\n",
        "                        loss_A = player_A.train(replay_memory)\n",
        "                        loss_sum_A += loss_A\n",
        "                        loss_step_A += 1\n",
        "                    next_state_A = None\n",
        "                \n",
        "                if next_state_B != None:\n",
        "                    replay_memory.push(state_B.unsqueeze(0), action_B, next_state_B, torch.tensor([reward_B], device=DEVICE))\n",
        "                    if len(replay_memory) >= replay_memory.batch_size:\n",
        "                        _ = player_B.train(replay_memory)\n",
        "                    next_state_B = None\n",
        "                          \n",
        "            if end:\n",
        "                # Once ending the game, no matter which player plays first\n",
        "                # Update is the same\n",
        "                if env.winner == player_A.player:\n",
        "                    # If opt wins the game, reward is guaranteed to be update-to-date\n",
        "                    reward_A = 1\n",
        "                    reward_B = -1\n",
        "                elif env.winner == player_B.player:\n",
        "                    reward_A = -1\n",
        "                    reward_B = 1\n",
        "                else:\n",
        "                    # Draw\n",
        "                    reward_A = 0\n",
        "                    reward_B = 0\n",
        "                \n",
        "                # If one agent wins the game, we don't actually real next_state\n",
        "                # Because q(s', a') will be cancelled out.\n",
        "                next_state_A = None\n",
        "                next_state_B = None\n",
        "                \n",
        "                replay_memory.push(state_A.unsqueeze(0), action_A, next_state_A, torch.tensor([reward_A], device=DEVICE))\n",
        "                replay_memory.push(state_B.unsqueeze(0), action_B, next_state_B, torch.tensor([reward_B], device=DEVICE))\n",
        "                \n",
        "                # Update target model every 500 epoch.\n",
        "                update_target = False\n",
        "\n",
        "                if (i+1) % 500 == 0:\n",
        "                    update_target = True\n",
        "\n",
        "                # Training model when game over, and replay_memory size >= batch size\n",
        "                if len(replay_memory) >= replay_memory.batch_size:\n",
        "                    loss_A = player_A.train(replay_memory, update_target)\n",
        "                    loss_sum_A += loss_A\n",
        "                    loss_step_A += 1\n",
        "                    _ = player_B.train(replay_memory, update_target)\n",
        "\n",
        "                    \n",
        "                # always focus on the average reward of player_A.\n",
        "                reward_sum_A += reward_A\n",
        "                env.reset()\n",
        "                break       \n",
        "            \n",
        "        #############################\n",
        "        ######### Plot ##############\n",
        "\n",
        "        if (i+1) % plot_interval == 0:\n",
        "            # calculate average reward at the end of the current interval.\n",
        "            average_reward_A = reward_sum_A / plot_interval  \n",
        "            average_loss_A = loss_sum_A / loss_step_A\n",
        "            print(average_reward_A)  \n",
        "            print(average_loss_A)\n",
        "            \n",
        "            M_opt_1 = validation(0, 0, player_A, epoch=500)\n",
        "            M_rand_1 = validation(1, 0, player_A, epoch=500)\n",
        "\n",
        "            idx = len(fig_widge.data)-2\n",
        "            fig_widge.data[idx].x = np.append(fig_widge.data[idx].x, i+1)[0:]\n",
        "            fig_widge.data[idx].y = np.append(fig_widge.data[idx].y, M_opt_1)[0:]\n",
        "            fig_widge.data[idx+1].x = np.append(fig_widge.data[idx+1].x, i+1)[0:]\n",
        "            fig_widge.data[idx+1].y = np.append(fig_widge.data[idx+1].y, M_rand_1)[0:]\n",
        "            fig_widge.layout.title.text = f'Epoch {i+1}, M_opt = {M_opt_1}, M_rand = {M_rand_1}'\n",
        "\n",
        "            average_rewards_A.append(average_reward_A)\n",
        "            average_losses_A.append(average_loss_A)\n",
        "            # reset reward_sum.\n",
        "            reward_sum_A = 0\n",
        "            loss_sum_A = 0\n",
        "            loss_step_A = 0\n",
        "\n",
        "    return (shared_model, replay_memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "9AvVt2EdjrUK",
      "metadata": {
        "id": "9AvVt2EdjrUK"
      },
      "outputs": [],
      "source": [
        "def generate_text(lst):\n",
        "    text = ['X' if x == 1 else 'O' if x == -1 else '' for x in lst]\n",
        "    return np.flip(np.reshape(text, (3, 3)), 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "KsrEGmDYjryG",
      "metadata": {
        "id": "KsrEGmDYjryG"
      },
      "outputs": [],
      "source": [
        "def retrieve_avail_qv(q_values, states):\n",
        "    return [val if (states[idx] == 0) else 0 for (idx, val) in enumerate(q_values)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e41f0e6",
      "metadata": {
        "id": "0e41f0e6"
      },
      "source": [
        "Question 11."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe589738",
      "metadata": {
        "id": "fe589738"
      },
      "outputs": [],
      "source": [
        "fw1 = render_figure_widge('Reward+Loss')\n",
        "fw1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7f1087b-dcb9-4927-a855-10dc7d75c32d",
      "metadata": {
        "id": "d7f1087b-dcb9-4927-a855-10dc7d75c32d"
      },
      "outputs": [],
      "source": [
        "epsilon_opt = 0.5  # This epsilon value represents exploration level of optimal player. Fixed in Question 1.\n",
        "epsilon_learn = [0.0, 0.2, 0.4, 0.8]  # This epsilon value represents exploration level of Q-learning agent. Defind by ourselves.\n",
        "epoch = 20000  # number of games to play in Question 1.\n",
        "plot_interval = 250  # The interval of games to calculate an average reward.\n",
        "for idx, epsilon in enumerate(epsilon_learn):\n",
        "    run_simulation(\n",
        "        epsilon_opt=epsilon_opt,\n",
        "        epsilon_learn_init=epsilon, \n",
        "        fig_widge=fw1,\n",
        "        colour=colours[idx],\n",
        "        mode='epsilon'\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bdd4c93",
      "metadata": {
        "id": "0bdd4c93"
      },
      "source": [
        "Question 12."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "df3f5fc5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537,
          "referenced_widgets": [
            "66eb89e1c0104a94ad594f6ba2f36fce"
          ]
        },
        "id": "df3f5fc5",
        "outputId": "76862377-7306-4114-ce14-8e6bd9752f39"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "66eb89e1c0104a94ad594f6ba2f36fce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FigureWidget({\n",
              "    'data': [],\n",
              "    'layout': {'annotations': [{'font': {'size': 16},\n",
              "                         â€¦"
            ]
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/a8874ba6619b6106/manager.min.js"
                }
              }
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "fw2 = render_figure_widge('Reward+Loss')\n",
        "fw2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "7266b52d",
      "metadata": {
        "id": "7266b52d"
      },
      "outputs": [],
      "source": [
        "epsilon_opt = 0.5  # This epsilon value represents exploration level of optimal player. Fixed in Question 1.\n",
        "epsilon_learn = [0.0, 0.2, 0.4, 0.8]  # This epsilon value represents exploration level of Q-learning agent. Defind by ourselves.\n",
        "epoch = 20000  # number of games to play in Question 1.\n",
        "plot_interval = 250  # The interval of games to calculate an average reward.\n",
        "for idx, epsilon in enumerate(epsilon_learn):\n",
        "    run_simulation(\n",
        "        epsilon_opt=epsilon_opt,\n",
        "        buffer_capacity=1,\n",
        "        batch_size=1,\n",
        "        epsilon_learn_init=epsilon, \n",
        "        fig_widge=fw2,\n",
        "        colour=colours[idx],\n",
        "        mode='epsilon'\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf023286",
      "metadata": {
        "id": "cf023286"
      },
      "source": [
        "Question 13."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "529107b6-0fa6-4c7e-96a3-fb323311bf0a",
      "metadata": {
        "id": "529107b6-0fa6-4c7e-96a3-fb323311bf0a"
      },
      "outputs": [],
      "source": [
        "fw3 = render_figure_widge()\n",
        "fw3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeebfa18-d036-4643-a683-63d4f884c859",
      "metadata": {
        "id": "aeebfa18-d036-4643-a683-63d4f884c859"
      },
      "outputs": [],
      "source": [
        "epsilon_opt = 0.5  # This epsilon value represents exploration level of optimal player.\n",
        "n_stars = [1, 100, 1000, 2000, 5000, 10000, 20000, 40000]\n",
        "epoch = 20000  # number of games to play in Question 1.\n",
        "plot_interval = 250  # The interval of games to calculate an average reward.\n",
        "for idx, n_s in enumerate(n_stars):\n",
        "    run_simulation(\n",
        "        epsilon_opt=epsilon_opt, \n",
        "        n_star=n_s,\n",
        "        epoch=epoch, \n",
        "        plot_interval=plot_interval, \n",
        "        colour=colours[idx%len(colours)], \n",
        "        fig_widge=fw3, \n",
        "        mode=\"validate n*\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AJyy2S_nE18U",
      "metadata": {
        "id": "AJyy2S_nE18U"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3VAe8vK0E18Z",
      "metadata": {
        "id": "3VAe8vK0E18Z"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "output.disable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ke-JVjUOoDJ9",
      "metadata": {
        "id": "ke-JVjUOoDJ9"
      },
      "source": [
        "Question 14"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f39MJL2IoFML",
      "metadata": {
        "id": "f39MJL2IoFML"
      },
      "outputs": [],
      "source": [
        "fw4 = render_figure_widge()\n",
        "fw4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7nAZlZiAoIMe",
      "metadata": {
        "id": "7nAZlZiAoIMe"
      },
      "outputs": [],
      "source": [
        "epsilon_opts = [.0, .2, .4, .6, .8]\n",
        "best_n_star = 1000\n",
        "for idx, epsilon_opt in enumerate(epsilon_opts):\n",
        "    run_simulation(\n",
        "        epsilon_opt=epsilon_opt, \n",
        "        n_star=best_n_star,\n",
        "        fig_widge=fw4,\n",
        "        colour=colours[idx%len(colours)],\n",
        "        mode='validate epsilon opt'\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7fOKfQ2op4k",
      "metadata": {
        "id": "c7fOKfQ2op4k"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "pQ_nLUQwnZuD",
      "metadata": {
        "id": "pQ_nLUQwnZuD"
      },
      "source": [
        "Question 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NsjlRmABnbWb",
      "metadata": {
        "id": "NsjlRmABnbWb"
      },
      "outputs": [],
      "source": [
        "fw5 = render_figure_widge('validation')\n",
        "fw5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZthOL_HLne4N",
      "metadata": {
        "id": "ZthOL_HLne4N"
      },
      "outputs": [],
      "source": [
        "epsilon_learn = [0, 0.2, 0.4, 0.6, 0.8]\n",
        "for idx, epsilon in enumerate(epsilon_learn):\n",
        "    self_practice_simulation(\n",
        "        colour=colours[idx%len(colours)],\n",
        "        epsilon_learn_init=epsilon,\n",
        "        fig_widge=fw5,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xXjgfuq0nj8s",
      "metadata": {
        "id": "xXjgfuq0nj8s"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "Nixtwuw8oqG6",
      "metadata": {
        "id": "Nixtwuw8oqG6"
      },
      "source": [
        "Question 17"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T0aGwttdo3Gq",
      "metadata": {
        "id": "T0aGwttdo3Gq"
      },
      "outputs": [],
      "source": [
        "fw6 = render_figure_widge('validation')\n",
        "fw6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fRIJL6Cto4rm",
      "metadata": {
        "id": "fRIJL6Cto4rm"
      },
      "outputs": [],
      "source": [
        "n_stars = [1, 100, 1000, 5000, 10000, 20000]\n",
        "libraries = []\n",
        "\n",
        "for idx, n_s in enumerate(n_stars):\n",
        "    libraries.append(self_practice_simulation(colour=colours[idx%len(colours)],n_star=n_s,fig_widge=fw6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cJGQUOO6jhe_",
      "metadata": {
        "id": "cJGQUOO6jhe_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "Xa0Wy7C-jh_B",
      "metadata": {
        "id": "Xa0Wy7C-jh_B"
      },
      "source": [
        "Question 19 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JLnzJ5pokKBL",
      "metadata": {
        "id": "JLnzJ5pokKBL"
      },
      "outputs": [],
      "source": [
        "# Retrieve FCN model for qvalue prediction and memory for sampling\n",
        "model, memory = self_practice_simulation(colour='black',n_star=1000,fig_widge=render_figure_widge('validation'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q5Ygphb1jj07",
      "metadata": {
        "id": "Q5Ygphb1jj07"
      },
      "outputs": [],
      "source": [
        "fig = make_subplots(rows=1, cols=3)\n",
        "\n",
        "for i in range(3):\n",
        "    random_sample = memory.sample(1)\n",
        "    state  = random_sample[0].state\n",
        "    grid = state[0][0] - state[0][1]\n",
        "    lst = grid_to_state(grid.cpu().detach().numpy())\n",
        "\n",
        "    qvalue = model(state)[0].cpu().tolist()\n",
        "    \n",
        "    fig.add_trace(go.Heatmap(z=np.flip(np.reshape(retrieve_avail_qv(qvalue, lst), (3,3)), 0),\n",
        "                         text= generate_text(lst), texttemplate=\"%{text}\",\n",
        "                         textfont={\"size\":20}, coloraxis='coloraxis'), row=1, col=i+1)\n",
        "    \n",
        "fig.update_layout(width=1200, height=500)\n",
        "fig.update_xaxes(visible=False)       \n",
        "fig.update_yaxes(visible=False)\n",
        "fig.update_layout(coloraxis=dict(colorscale='RdBu', colorbar_thickness=23))\n",
        "fig.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Deep_Q_full_better_comment.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "65a2bc8204bd30e294b2f94e2545ee4e57c8e2de41c09c84b6b285528f35f0a1"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "66eb89e1c0104a94ad594f6ba2f36fce": {
          "model_module": "jupyterlab-plotly",
          "model_module_version": "^5.5.0",
          "model_name": "FigureModel",
          "state": {
            "_config": {
              "plotlyServerURL": "https://plot.ly"
            },
            "_data": [],
            "_dom_classes": [],
            "_js2py_layoutDelta": null,
            "_js2py_pointsCallback": null,
            "_js2py_relayout": {},
            "_js2py_restyle": {},
            "_js2py_traceDeltas": null,
            "_js2py_update": {},
            "_last_layout_edit_id": 1288,
            "_last_trace_edit_id": 1288,
            "_layout": {
              "annotations": [
                {
                  "font": {
                    "size": 16
                  },
                  "showarrow": false,
                  "text": "Average Reward",
                  "x": 0.225,
                  "xanchor": "center",
                  "xref": "paper",
                  "y": 1,
                  "yanchor": "bottom",
                  "yref": "paper"
                },
                {
                  "font": {
                    "size": 16
                  },
                  "showarrow": false,
                  "text": "Training Loss",
                  "x": 0.775,
                  "xanchor": "center",
                  "xref": "paper",
                  "y": 1,
                  "yanchor": "bottom",
                  "yref": "paper"
                }
              ],
              "height": 500,
              "hovermode": "x unified",
              "template": {
                "data": {
                  "bar": [
                    {
                      "error_x": {
                        "color": "#2a3f5f"
                      },
                      "error_y": {
                        "color": "#2a3f5f"
                      },
                      "marker": {
                        "line": {
                          "color": "#E5ECF6",
                          "width": 0.5
                        },
                        "pattern": {
                          "fillmode": "overlay",
                          "size": 10,
                          "solidity": 0.2
                        }
                      },
                      "type": "bar"
                    }
                  ],
                  "barpolar": [
                    {
                      "marker": {
                        "line": {
                          "color": "#E5ECF6",
                          "width": 0.5
                        },
                        "pattern": {
                          "fillmode": "overlay",
                          "size": 10,
                          "solidity": 0.2
                        }
                      },
                      "type": "barpolar"
                    }
                  ],
                  "carpet": [
                    {
                      "aaxis": {
                        "endlinecolor": "#2a3f5f",
                        "gridcolor": "white",
                        "linecolor": "white",
                        "minorgridcolor": "white",
                        "startlinecolor": "#2a3f5f"
                      },
                      "baxis": {
                        "endlinecolor": "#2a3f5f",
                        "gridcolor": "white",
                        "linecolor": "white",
                        "minorgridcolor": "white",
                        "startlinecolor": "#2a3f5f"
                      },
                      "type": "carpet"
                    }
                  ],
                  "choropleth": [
                    {
                      "colorbar": {
                        "outlinewidth": 0,
                        "ticks": ""
                      },
                      "type": "choropleth"
                    }
                  ],
                  "contour": [
                    {
                      "colorbar": {
                        "outlinewidth": 0,
                        "ticks": ""
                      },
                      "colorscale": [
                        [
                          0,
                          "#0d0887"
                        ],
                        [
                          0.1111111111111111,
                          "#46039f"
                        ],
                        [
                          0.2222222222222222,
                          "#7201a8"
                        ],
                        [
                          0.3333333333333333,
                          "#9c179e"
                        ],
                        [
                          0.4444444444444444,
                          "#bd3786"
                        ],
                        [
                          0.5555555555555556,
                          "#d8576b"
                        ],
                        [
                          0.6666666666666666,
                          "#ed7953"
                        ],
                        [
                          0.7777777777777778,
                          "#fb9f3a"
                        ],
                        [
                          0.8888888888888888,
                          "#fdca26"
                        ],
                        [
                          1,
                          "#f0f921"
                        ]
                      ],
                      "type": "contour"
                    }
                  ],
                  "contourcarpet": [
                    {
                      "colorbar": {
                        "outlinewidth": 0,
                        "ticks": ""
                      },
                      "type": "contourcarpet"
                    }
                  ],
                  "heatmap": [
                    {
                      "colorbar": {
                        "outlinewidth": 0,
                        "ticks": ""
                      },
                      "colorscale": [
                        [
                          0,
                          "#0d0887"
                        ],
                        [
                          0.1111111111111111,
                          "#46039f"
                        ],
                        [
                          0.2222222222222222,
                          "#7201a8"
                        ],
                        [
                          0.3333333333333333,
                          "#9c179e"
                        ],
                        [
                          0.4444444444444444,
                          "#bd3786"
                        ],
                        [
                          0.5555555555555556,
                          "#d8576b"
                        ],
                        [
                          0.6666666666666666,
                          "#ed7953"
                        ],
                        [
                          0.7777777777777778,
                          "#fb9f3a"
                        ],
                        [
                          0.8888888888888888,
                          "#fdca26"
                        ],
                        [
                          1,
                          "#f0f921"
                        ]
                      ],
                      "type": "heatmap"
                    }
                  ],
                  "heatmapgl": [
                    {
                      "colorbar": {
                        "outlinewidth": 0,
                        "ticks": ""
                      },
                      "colorscale": [
                        [
                          0,
                          "#0d0887"
                        ],
                        [
                          0.1111111111111111,
                          "#46039f"
                        ],
                        [
                          0.2222222222222222,
                          "#7201a8"
                        ],
                        [
                          0.3333333333333333,
                          "#9c179e"
                        ],
                        [
                          0.4444444444444444,
                          "#bd3786"
                        ],
                        [
                          0.5555555555555556,
                          "#d8576b"
                        ],
                        [
                          0.6666666666666666,
                          "#ed7953"
                        ],
                        [
                          0.7777777777777778,
                          "#fb9f3a"
                        ],
                        [
                          0.8888888888888888,
                          "#fdca26"
                        ],
                        [
                          1,
                          "#f0f921"
                        ]
                      ],
                      "type": "heatmapgl"
                    }
                  ],
                  "histogram": [
                    {
                      "marker": {
                        "pattern": {
                          "fillmode": "overlay",
                          "size": 10,
                          "solidity": 0.2
                        }
                      },
                      "type": "histogram"
                    }
                  ],
                  "histogram2d": [
                    {
                      "colorbar": {
                        "outlinewidth": 0,
                        "ticks": ""
                      },
                      "colorscale": [
                        [
                          0,
                          "#0d0887"
                        ],
                        [
                          0.1111111111111111,
                          "#46039f"
                        ],
                        [
                          0.2222222222222222,
                          "#7201a8"
                        ],
                        [
                          0.3333333333333333,
                          "#9c179e"
                        ],
                        [
                          0.4444444444444444,
                          "#bd3786"
                        ],
                        [
                          0.5555555555555556,
                          "#d8576b"
                        ],
                        [
                          0.6666666666666666,
                          "#ed7953"
                        ],
                        [
                          0.7777777777777778,
                          "#fb9f3a"
                        ],
                        [
                          0.8888888888888888,
                          "#fdca26"
                        ],
                        [
                          1,
                          "#f0f921"
                        ]
                      ],
                      "type": "histogram2d"
                    }
                  ],
                  "histogram2dcontour": [
                    {
                      "colorbar": {
                        "outlinewidth": 0,
                        "ticks": ""
                      },
                      "colorscale": [
                        [
                          0,
                          "#0d0887"
                        ],
                        [
                          0.1111111111111111,
                          "#46039f"
                        ],
                        [
                          0.2222222222222222,
                          "#7201a8"
                        ],
                        [
                          0.3333333333333333,
                          "#9c179e"
                        ],
                        [
                          0.4444444444444444,
                          "#bd3786"
                        ],
                        [
                          0.5555555555555556,
                          "#d8576b"
                        ],
                        [
                          0.6666666666666666,
                          "#ed7953"
                        ],
                        [
                          0.7777777777777778,
                          "#fb9f3a"
                        ],
                        [
                          0.8888888888888888,
                          "#fdca26"
                        ],
                        [
                          1,
                          "#f0f921"
                        ]
                      ],
                      "type": "histogram2dcontour"
                    }
                  ],
                  "mesh3d": [
                    {
                      "colorbar": {
                        "outlinewidth": 0,
                        "ticks": ""
                      },
                      "type": "mesh3d"
                    }
                  ],
                  "parcoords": [
                    {
                      "line": {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        }
                      },
                      "type": "parcoords"
                    }
                  ],
                  "pie": [
                    {
                      "automargin": true,
                      "type": "pie"
                    }
                  ],
                  "scatter": [
                    {
                      "marker": {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        }
                      },
                      "type": "scatter"
                    }
                  ],
                  "scatter3d": [
                    {
                      "line": {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        }
                      },
                      "marker": {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        }
                      },
                      "type": "scatter3d"
                    }
                  ],
                  "scattercarpet": [
                    {
                      "marker": {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        }
                      },
                      "type": "scattercarpet"
                    }
                  ],
                  "scattergeo": [
                    {
                      "marker": {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        }
                      },
                      "type": "scattergeo"
                    }
                  ],
                  "scattergl": [
                    {
                      "marker": {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        }
                      },
                      "type": "scattergl"
                    }
                  ],
                  "scattermapbox": [
                    {
                      "marker": {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        }
                      },
                      "type": "scattermapbox"
                    }
                  ],
                  "scatterpolar": [
                    {
                      "marker": {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        }
                      },
                      "type": "scatterpolar"
                    }
                  ],
                  "scatterpolargl": [
                    {
                      "marker": {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        }
                      },
                      "type": "scatterpolargl"
                    }
                  ],
                  "scatterternary": [
                    {
                      "marker": {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        }
                      },
                      "type": "scatterternary"
                    }
                  ],
                  "surface": [
                    {
                      "colorbar": {
                        "outlinewidth": 0,
                        "ticks": ""
                      },
                      "colorscale": [
                        [
                          0,
                          "#0d0887"
                        ],
                        [
                          0.1111111111111111,
                          "#46039f"
                        ],
                        [
                          0.2222222222222222,
                          "#7201a8"
                        ],
                        [
                          0.3333333333333333,
                          "#9c179e"
                        ],
                        [
                          0.4444444444444444,
                          "#bd3786"
                        ],
                        [
                          0.5555555555555556,
                          "#d8576b"
                        ],
                        [
                          0.6666666666666666,
                          "#ed7953"
                        ],
                        [
                          0.7777777777777778,
                          "#fb9f3a"
                        ],
                        [
                          0.8888888888888888,
                          "#fdca26"
                        ],
                        [
                          1,
                          "#f0f921"
                        ]
                      ],
                      "type": "surface"
                    }
                  ],
                  "table": [
                    {
                      "cells": {
                        "fill": {
                          "color": "#EBF0F8"
                        },
                        "line": {
                          "color": "white"
                        }
                      },
                      "header": {
                        "fill": {
                          "color": "#C8D4E3"
                        },
                        "line": {
                          "color": "white"
                        }
                      },
                      "type": "table"
                    }
                  ]
                },
                "layout": {
                  "annotationdefaults": {
                    "arrowcolor": "#2a3f5f",
                    "arrowhead": 0,
                    "arrowwidth": 1
                  },
                  "autotypenumbers": "strict",
                  "coloraxis": {
                    "colorbar": {
                      "outlinewidth": 0,
                      "ticks": ""
                    }
                  },
                  "colorscale": {
                    "diverging": [
                      [
                        0,
                        "#8e0152"
                      ],
                      [
                        0.1,
                        "#c51b7d"
                      ],
                      [
                        0.2,
                        "#de77ae"
                      ],
                      [
                        0.3,
                        "#f1b6da"
                      ],
                      [
                        0.4,
                        "#fde0ef"
                      ],
                      [
                        0.5,
                        "#f7f7f7"
                      ],
                      [
                        0.6,
                        "#e6f5d0"
                      ],
                      [
                        0.7,
                        "#b8e186"
                      ],
                      [
                        0.8,
                        "#7fbc41"
                      ],
                      [
                        0.9,
                        "#4d9221"
                      ],
                      [
                        1,
                        "#276419"
                      ]
                    ],
                    "sequential": [
                      [
                        0,
                        "#0d0887"
                      ],
                      [
                        0.1111111111111111,
                        "#46039f"
                      ],
                      [
                        0.2222222222222222,
                        "#7201a8"
                      ],
                      [
                        0.3333333333333333,
                        "#9c179e"
                      ],
                      [
                        0.4444444444444444,
                        "#bd3786"
                      ],
                      [
                        0.5555555555555556,
                        "#d8576b"
                      ],
                      [
                        0.6666666666666666,
                        "#ed7953"
                      ],
                      [
                        0.7777777777777778,
                        "#fb9f3a"
                      ],
                      [
                        0.8888888888888888,
                        "#fdca26"
                      ],
                      [
                        1,
                        "#f0f921"
                      ]
                    ],
                    "sequentialminus": [
                      [
                        0,
                        "#0d0887"
                      ],
                      [
                        0.1111111111111111,
                        "#46039f"
                      ],
                      [
                        0.2222222222222222,
                        "#7201a8"
                      ],
                      [
                        0.3333333333333333,
                        "#9c179e"
                      ],
                      [
                        0.4444444444444444,
                        "#bd3786"
                      ],
                      [
                        0.5555555555555556,
                        "#d8576b"
                      ],
                      [
                        0.6666666666666666,
                        "#ed7953"
                      ],
                      [
                        0.7777777777777778,
                        "#fb9f3a"
                      ],
                      [
                        0.8888888888888888,
                        "#fdca26"
                      ],
                      [
                        1,
                        "#f0f921"
                      ]
                    ]
                  },
                  "colorway": [
                    "#636efa",
                    "#EF553B",
                    "#00cc96",
                    "#ab63fa",
                    "#FFA15A",
                    "#19d3f3",
                    "#FF6692",
                    "#B6E880",
                    "#FF97FF",
                    "#FECB52"
                  ],
                  "font": {
                    "color": "#2a3f5f"
                  },
                  "geo": {
                    "bgcolor": "white",
                    "lakecolor": "white",
                    "landcolor": "#E5ECF6",
                    "showlakes": true,
                    "showland": true,
                    "subunitcolor": "white"
                  },
                  "hoverlabel": {
                    "align": "left"
                  },
                  "hovermode": "closest",
                  "mapbox": {
                    "style": "light"
                  },
                  "paper_bgcolor": "white",
                  "plot_bgcolor": "#E5ECF6",
                  "polar": {
                    "angularaxis": {
                      "gridcolor": "white",
                      "linecolor": "white",
                      "ticks": ""
                    },
                    "bgcolor": "#E5ECF6",
                    "radialaxis": {
                      "gridcolor": "white",
                      "linecolor": "white",
                      "ticks": ""
                    }
                  },
                  "scene": {
                    "xaxis": {
                      "backgroundcolor": "#E5ECF6",
                      "gridcolor": "white",
                      "gridwidth": 2,
                      "linecolor": "white",
                      "showbackground": true,
                      "ticks": "",
                      "zerolinecolor": "white"
                    },
                    "yaxis": {
                      "backgroundcolor": "#E5ECF6",
                      "gridcolor": "white",
                      "gridwidth": 2,
                      "linecolor": "white",
                      "showbackground": true,
                      "ticks": "",
                      "zerolinecolor": "white"
                    },
                    "zaxis": {
                      "backgroundcolor": "#E5ECF6",
                      "gridcolor": "white",
                      "gridwidth": 2,
                      "linecolor": "white",
                      "showbackground": true,
                      "ticks": "",
                      "zerolinecolor": "white"
                    }
                  },
                  "shapedefaults": {
                    "line": {
                      "color": "#2a3f5f"
                    }
                  },
                  "ternary": {
                    "aaxis": {
                      "gridcolor": "white",
                      "linecolor": "white",
                      "ticks": ""
                    },
                    "baxis": {
                      "gridcolor": "white",
                      "linecolor": "white",
                      "ticks": ""
                    },
                    "bgcolor": "#E5ECF6",
                    "caxis": {
                      "gridcolor": "white",
                      "linecolor": "white",
                      "ticks": ""
                    }
                  },
                  "title": {
                    "x": 0.05
                  },
                  "xaxis": {
                    "automargin": true,
                    "gridcolor": "white",
                    "linecolor": "white",
                    "ticks": "",
                    "title": {
                      "standoff": 15
                    },
                    "zerolinecolor": "white",
                    "zerolinewidth": 2
                  },
                  "yaxis": {
                    "automargin": true,
                    "gridcolor": "white",
                    "linecolor": "white",
                    "ticks": "",
                    "title": {
                      "standoff": 15
                    },
                    "zerolinecolor": "white",
                    "zerolinewidth": 2
                  }
                }
              },
              "width": 1400,
              "xaxis": {
                "anchor": "y",
                "domain": [
                  0,
                  0.45
                ]
              },
              "xaxis2": {
                "anchor": "y2",
                "domain": [
                  0.55,
                  1
                ]
              },
              "yaxis": {
                "anchor": "x",
                "domain": [
                  0,
                  1
                ]
              },
              "yaxis2": {
                "anchor": "x2",
                "domain": [
                  0,
                  1
                ]
              }
            },
            "_model_module": "jupyterlab-plotly",
            "_model_module_version": "^5.5.0",
            "_model_name": "FigureModel",
            "_py2js_addTraces": null,
            "_py2js_animate": {},
            "_py2js_deleteTraces": {},
            "_py2js_moveTraces": {},
            "_py2js_relayout": {},
            "_py2js_removeLayoutProps": {},
            "_py2js_removeTraceProps": {},
            "_py2js_restyle": null,
            "_py2js_update": {},
            "_view_count": 1,
            "_view_module": "jupyterlab-plotly",
            "_view_module_version": "^5.5.0",
            "_view_name": "FigureView"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
